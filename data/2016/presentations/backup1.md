## Un Monde de Données

* Cours n°1: Big data etc.

===

## Parcours

* Guillaume Plique
* Formation: - Hypokhâgne - SciencesPo - Waseda Daigaku, Tokyo - HETIC
* Emploi: - Conseils généraux - Archives Nationales - médialab de SciencesPo

===

## Métier

* Développeur
* Domaines: - Analyse visuelle des réseaux - Traitement automatique du langage & logique floue - Web mining (crawling & scraping)
* Langages: - JavaScript, Python, Ruby, Clojure

===

## Le médialab

* Laboratoire de recherche rassemblant les profils suivants:  - Des chercheurs en Sciences humaines  - Des designers  - Des ingénieurs

===

## Concept du cours

* Un Monde de Données
* 4 thématiques: - Le Big Data - Le traitement de données - La dataviz - L’Open Data

===

## Objectif

* Vous donner des concepts et vous ouvrir à des thématiques parfois obscures (voire fumeux).
* Donner un visage à des mots entendus tous les jours mais pourtant vaporeux: Big Data, Data mining, Web mining, Machine learning etc.
* Vous montrer que ces choses existent pour qu’elles deviennent de nouveaux vecteurs professionnels pour vous.

===

## Sic transit gloria mundi

* Gardez à l’esprit que je ne vous donne ici que des pistes à explorer et qu’il est impossible d’être exhaustif.
* Cela va être dense et parfois même légèrement technique. N’ayez pas peur des digressions.
* Enfin, faites attention à l’emploi des mots sur lesquels je vais tenter de mettre une définition: ce sont pour la plupart des buzzwords rarement utilisés à bon escient.

===

## Les règles du cours

* N’hésitez pas à poser des questions.
* N’hésitez pas à m’interrompre si vous trouvez que je raconte des inepties.
* Vous êtes autorisés à me lancer des fournitures de bureau si:   1. Je parle trop vite.   2. J’utilise un vocabulaire acroamatique ou abscons.   3. Je deviens absurde

===

## Interlude musical

* Combien de développeurs (langages)?
* Combien de designers?
* Quid des CDP et alii?
* Qui a une expérience ayant un lien avec les sujets du cours?
* Qui a fait du latin ou du grec ancien?
* Déjà eu des cours de M. Topolov?

===

## Big Data?

* « Big Data is like teenage sex:  - Everyone talks about it, - nobody really knows how to do it, - everyone thinks everyone else is doing it, - so everyone claims they are doing it… » 

===

## Un peu d’étymologie

* Data vient du supin latin Datum (du verbe Do, Dare).
* Cela signifie littéralement « ce qui est donné ».
* Data est le pluriel de Datum.

===

## Une définition?

* Description élémentaire d’une réalité.
* En informatique, cette description est codée et revêt un aspect intelligible au travers de la logique de l’ordinateur.
* Tout un pan de la science informatique consiste à structurer la donnée.

===

## Donné vs. construit

* De nombreux intellectuels hargneux ont tendance à rappeler que ce que nous appelons données sont en réalité des construits.
* Un donné, conceptuellement, est-ce qui apparait immédiatement à l’intellect avant que celui-ci n’y impose ses procédés d’élaboration.
* La délimitation des deux concepts est, par ailleurs, un très grand problème philosophique.
* Mais ‘base de construits’ ne sonnait pas bien.

===

## Quid du Big data?

* Qu’est-ce que le Big data? (un peu d’anglais?)
* Un amas extrêmement important de données.
* Tant que cela tient sur vos ordinateurs, on ne parlera pas de Big data.
* Donc, trivialement, pas la table SQL de 1000 lignes de tata Lucienne.

===

## Le saut quantitatif

* Le nombre de données collectées croît aujourd’hui de manière exponentielle.

===

## Le saut quantitatif

* L’humanité a produit dans les trois dernières années plus de données que ce qu’elle a pu produire depuis l’origine de l’écriture. (Exponentielle, quand tu nous tiens).
* 90% des données mondiales ont été produites les deux dernières années (autre manière de le dire).

===

## Le saut quantitatif

* Des nombres en valant d’autres:  - 1 zettaoctet (1021) franchi en 2011 - 204 millions d’emails/minute - 40 000 recherches sur Google à la seconde - 100 heures de vidéo sur YouTube/minute - 720 nouveaux sites web par jour

===

## Le saut quantitatif

* En réalité, tout cela est proprement inquantifiable et les nombres varient selon les source.
* La seule chose sur laquelle les sources s’accordent est le côté Big.

===

## Le saut quantitatif

* Cela n’est pas prêt de ralentir:   - Quantified self   - Un bon avion (25 To/heure)   - Objets connectés   - Capacité de stockage de plus en plus évidente
* Seul l’arrêt brutal de la loi de Moore pourrait stopper cette accélération.

===

## Le saut quantitatif

* Or, si on commence à savoir stocker cette masse de données:
* 1) Les moyens techniques nous permettant d’analyser de tels volumes restent à inventer.
* 2) On ne sait pas toujours extraire le sens ces données (elles n’en ont pas forcément, par ailleurs).

===

## Le saut quantitatif

* Les deux enjeux du Big data sont donc les suivants:
* Les moyens techniques nécessaires au stockage et à l’analyse des données.
* L’appareil théorique permettant de donner un sens à ces données.

===

## Le saut quantitatif

* Etape importante de l’histoire de l’informatique.
* On sait que ces données et leur analyse peuvent changer la manière dont on perçoit le monde.
* Mais, les standards techniques et théoriques sont loins d’être arrêtés.
* Le Big data est encore dans une dynamique de pionniers.

===

## Les trois V

* Effort théorique de définition du Big data:
* 1) Volume
* 2) Variété
* 3) Vélocité

===

## Quelques exemples

* La Génétique

===

## Quelques exemples

* La Médecine

===

## Quelques exemples

* Facebook
* Linkedin

===

## Quelques exemples

* Facebook
* Linkedin

===

## Un marché en expansion

* Le Big data est une nouvelle source de profits substantiels.
* Une fois de plus, tous les organismes économiques y vont de leurs conjectures sur le sujet.
* Celles-ci sont toutes différentes et floues mais tout le monde s’accorde sur ce point: le Big data, c’est gros.
* Vous noterez la mise en abyme.

===

## Un marché en expansion

* Le Big data est une nouvelle source de profits substantiels.
* Une fois de plus, tous les organismes économiques y vont de leurs conjectures sur le sujet.
* Celles-ci sont toutes différentes et floues mais tout le monde s’accorde sur ce point: le Big data, c’est gros.
* Vous noterez la mise en abyme.

===

## Un marché en expansion

* Des nombres en valant d’autres (2, le retour):  - 4,2 millions d’emplois d’ici 2015 - 54,3 milliards de chiffre d’affaire horizon 2017
* Il est tout à fait possible de construire un business autour de la donnée (mais de moins en moins dans la vente de données).

===

## Mais.

* Le Big data n’est en aucun cas une idée neuve (déjà appliqué par les géants du web depuis au moins 10 ans).
* C’est surtout la médiatisation et la démocratisation du concept qui est impressionnante.

===

## Quantitatif & Qualitatif

* François Pérusse et la notion de seuil critique.
* Avec la quantité de données dont nous disposons, la résolution de certains problèmes devient possible dès qu’un seuil critique est atteint.
* Même si certaines données, observées de manière atomique, semblent inintéressantes sur le plan qualitatif, elles deviennent une mine d’or lorsqu’on en possède énormément et qu’on peut les croiser avec d’autres.

===

## Exemple

* Détecter le profil socio-démographique d’une personne grâce à ses headers HTTP:
* Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 Accept-Encoding:gzip, deflate, sdch Accept-Language:fr-FR,fr;q=0.8,en-US;q=0.6,en;q=0.4,es;q=0.2 Cache-Control:max-age=0 Connection:keep-alive Cookie:__qca=P0-1380333920-1417718780994 Host:www.moddb.com User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36

===

## Data vs. Déterminisme

* Le Big data et le machine learning nous permettent de changer de paradigme si besoin:
* Il est possible, dans certains cas, de résoudre un problème grâce aux données plutôt que grâce à un algorithme.
* La combinaison des deux approches reste évidemment possible et puissante.

===

## Illustration n°1

* Découper un mot en syllabes.
* Utiliser les règles phonétiques anglaises: V CV VC CVC CCV CCCV CVCC
* Ou utiliser un dictionnaire?
* (N.B.: Eternel problème de computation vs. mémoire)

===

#

* Les rainbow attacks.
* Une méthode issue de la cryptographie: les rainbow tables.
* Comment un script perl de deux lignes peut casser vos mots de passe?
* Vive les condiments!

===

## Illustration « it’s over 9000 »

* Pour résumer: masse de données = possibilité de résoudre des problèmes differemment.
* Stratégie de Google depuis le début, notamment.
* La recherche ne fonctionne plus par curation de liens et index mais bien par collecte des données du web lui même.
* Leur correction orthographique en est par ailleurs un pur produit. (Dérives)

===

## Au delà du Big data

* Le Big data est surtout le reflet d’une prise de conscience des acteurs et d’un changement de paradigme.
* Le Big data a évidemment des limites et ne pose que peu la question des données floues ou trop peu nombreuses (pourtant intéressantes mais encore plus complexes à analyser, paradoxalement).
* Parfois la richesse des données se trouve bien plus dans les croisements que dans la masse (datafrance).

===

## Big Data & Technique

* Trois axes (choisis arbitrairement):  - Le stockage des données - La parallélisation - Le machine learning

===

## Ye Olde SQL

* Structured Query Language (pas BDD)  SELECT * FROM table WHERE name = ‘Jonhatan’
* Origines théoriques dans les années 60. Edgar Frank Codd, 1970 Donald Chamberlain et Raymond Boyce, 1974
* Nombreux système de BDD utilisent le SQL: MySQL, ORACLE, PostgreSQL, MariaDB etc.
* Evolue encore (2011: tables temporelles, PERIOD FOR etc.)
* Problèmes: sur-architecturation, problèmes de scalabilité.

===

## Le paradigme NoSQL

===

## Le paradigme NoSQL

* Au commencement, tout était SQL.
* Il n’y avait qu’assez rarement l’occasion de se poser la question du format de stockage des données.
* Mais le SQL est trop limitant pour les usages contemporains.
* Chaque problème est différent et il convient d’utiliser les outils adaptés pour le résoudre.
* L’idéologie du tout SQL n’a donc plus de validité.

===

## Disclaimer

* Achtung! Il ne s’agit absolument pas de dire que le SQL est dépassé (ce serait une grossière erreur que de penser cela).
* Cependant, le SQL n’est qu’une possibilité parmi bien d’autres.
* Il est donc contre-productif d’utiliser le SQL partout, notamment dans des cas où il n’est pas efficace.

===

## Formats de données

* Exemple: Voudriez-vous toujours utiliser des fichiers Excel pour stocker des données?
* Quel genre de données avez-vous?: Données tabulaires, scalaires, non-scalaires?
* Question de la taille du fichier, aussi.
* Parfois le JSON, c’est pas bon et le CSV c’est à jeter.
* On veut le choix crévindiou (et surtout pas de xls)!

===

## CSV

* Exemple: Voudriez-vous toujours utiliser des fichiers Excel pour stocker des données?
* Quel genre de données avez-vous?: Données tabulaires, scalaires, non-scalaires?
* Question de la taille du fichier, aussi.
* Parfois le JSON, c’est pas bon et le CSV c’est à jeter.
* On veut le choix crévindiou!

===

## XML

* Exemple: Voudriez-vous toujours utiliser des fichiers Excel pour stocker des données?
* Quel genre de données avez-vous?: Données tabulaires, scalaires, non-scalaires?
* Question de la taille du fichier, aussi.
* Parfois le JSON, c’est pas bon et le CSV c’est à jeter.
* On veut le choix crévindiou!

===

## JSON

* Exemple: Voudriez-vous toujours utiliser des fichiers Excel pour stocker des données?
* Quel genre de données avez-vous?: Données tabulaires, scalaires, non-scalaires?
* Question de la taille du fichier, aussi.
* Parfois le JSON, c’est pas bon et le CSV c’est à jeter.
* On veut le choix crévindiou!

===

## YAML

* Exemple: Voudriez-vous toujours utiliser des fichiers Excel pour stocker des données?
* Quel genre de données avez-vous?: Données tabulaires, scalaires, non-scalaires?
* Question de la taille du fichier, aussi.
* Parfois le JSON, c’est pas bon et le CSV c’est à jeter.
* On veut le choix crévindiou!

===

## Le paradigme NoSQL

* Le NoSQL procède du même effort d’adaptation d’un outil à son usage.
* Le NoSQL représente donc tous les systèmes de base de données qui ne se basent pas sur SQL.
* Mais, le risque ici est de considérer que le NoSQL comme UN seul nouveau standard.
* Le NoSQL est plutôt une myriade de nouveaux standards.
* Le NoSQL c’est le far west.

===

## Le paradigme NoSQL

* Le NoSQL est doublement varié:
* 1) Les usages ciblés et les formats de données sont extrêmement divers.
* 2) Les standards ne sont pas établis et il n’est pas rare de trouver plusieurs systèmes de base de données en concurrence sur la même mouvance NoSQL.

===

## Le paradigme NoSQL

* Pour résumer, le NoSQL c’est:  - Un usage = un outil. - Une structure de données souvent plus proche du développeur. - Pas le SQL (sinon on appelle ça le SQL)
* Faisons un bref tour des systèmes NoSQL existants (volontairement non-exhaustif tant la variété de systèmes différents est grande).

===

## Orientées colonnes

* Problème à résoudre: la scalabilité.            Exemples: Cassandra, Accumulo

===

## Orientées documents

* Problèmes à résoudre:   - Surarchitecturation du SQL   - Rigidité des schemas
* Analogie développement: Un array JavaScript
* Exemples: MongoDB,CouchDB

===

#

* Le graphe comme structure de données:
* Des noeuds (type)
* Des arcs (prédicat)

===

## Orientées graphe

* Problème à résoudre: Gestion des données ultra-relationnelles.
* Exemples: Neo4j, Titan

===

## Le Cypher

* Exemple de langage de requêtage de graphe utilisé par Neo4j. (Matrix)
* MATCH (u:User)-[:LIKES]->(m:Movies) RETURN u,m LIMIT 30 ORDER BY m.title;

===

## Orientées clé-valeur / RAM

* Problèmes à résoudre:   - I/O, temps d’accès aux données   - Rigidité des schemas
* Analogie développement: Un objet JavaScript
* Exemples:    Redis, Memcache, Riak, Couchbase

===

## Orientées whatever

* Problèmes à résoudre:   - Besoin de plusieurs paradigmes en un.
* Exemples:    ArangoDB, OrientDB

===

## Dans le navigateur

* Il y a aussi du NoSQL dans le navigateur:  - localStorage (clé-valeur) - IndexedDB (clé-valeur & documents)
* Le WebSQL a été déprécié en faveur d’IndexedDB.
* PouchDB (synchronisation client-serveur)

===

## Suite & Fin?

* Quand il n’y en a plus, il y en a encore:   - Les bases orientées Flux   - Les bases serverless (SQLite en NoSQL)
* Le NoSQL arrive même dans certains bastions du SQL comme Postgre (Support du requêtage json etc.)
* Le NoSQL est un monde fabuleux. N’ayez pas peur de l’explorer pour découvrir de nouvelles solutions techniques à vos problèmes.

===

## La parallélisation

* Un des grands enjeux techniques du Big data.
* Le concept est cela dit loin d’être neuf et une bonne partie des calculs scientifiques complexes sont d’ores et déjà distribués.
* Divide & conquer
* Limitée aux algorithmes pouvant découper leur calculs. Exemple: une somme.

===

## La programmation fonctionnelle

* Il est coutume de dire qu’il existe trois paradigmes majeurs concernant la programmation:
* 1) Le paradigme impératif
* 2) Le paradigme objet
* 3) Le paradigme déclaratif

===

## Le lambda calcul

* Système inventé par Alonzo Church dans les années 1930 (les fonctions récursives, c’est lui).

===

## Pureté

* Une fonction est dite pure lorsque que l’on est certain qu’elle est libre de tout effet de bord et qu’aucun élément extérieur ne peut affecter son exécution (et inversement).
* En d’autres termes, une fonction est pure lorsque l’on est certain que si l’on passe un jeu d’argument donné à la fonction, celle-ci renverra toujours, quoi qu’il arrive, la même chose.

===

## Exemple

===

## Immutabilité

* En programmation fonctionnelle, la mutabilité n’existe pas.
* Cela veut dire qu’une variable ne peut jamais, muter, changer.
* Ainsi, pour « changer » une variable, il faut en créer une nouvelle différente.
* Cela abolit toute forme d’incertitude concernant la valeur d’une variable au prix d’une plus grande rigueur.
* A ce titre, la notion de pointeur ou de référence est absente de la programmation fonctionnelle et toutes les variables sont passées par valeur.

===

## Exemple

===

## Fonctions de premier ordre

* En programmation fonctionnelle, les fonctions sont considérées comme des valeurs à part entière et non pas comme des raccourcis syntaxiques.
* Cela veut dire que l’on peut très bien passer des fonctions comme arguments à d’autres fonctions et agir sur les fonctions elles-mêmes (Yo dawg…).
* On appelle donc fonctions de premier ordre des fonctions prenant des fonctions comme argument.

===

## Exemple

===

## Les langages fonctionnels

* Lisp (le plus ancien, 1958), Clojure etc.
* Haskell, Erlang etc.

===

## Un peu de Clojure?

===

## Je m’en sers déjà?

* En réalité, la plupart des langages de haut niveau utilisent aujourd’hui plusieurs paradigmes à la fois.
* Le JavaScript, le python, le ruby et même le php (sigh.) utilisent déjà beaucoup de concepts de la programmation fonctionnelle tout en restant près de l’objet et de l’impératif.
* Pourquoi cette longue digression?: MapReduce

===

## MapReduce

* Processus permettant de paralléliser des calculs/traitements concernant de grandes masses de données.
* « Inventé » par Google.
* Démocratisation via Hadoop.
* GFS (64mb chunks), HDFS.
* Comme son nom l’indique: map, puis reduce.

===

## L’explication en image

* Processus permettant de paralléliser des calculs/traitements concernant de grandes masses de données.
* « Inventé » par Google.
* Démocratisation via Hadoop.
* GFS (65mb chunks), HDFS.
* Comme son nom l’indique: map, puis reduce.

===

## MapReduce

* La granularité des tâche permet de rendre le tout fiable et scalable. (Tolérance à l’erreur)
* Son objectif est de rendre les problématiques de parallélisation invisibles à l’auteur du calcul.
* Nombreuses implémentations existantes, plus un concept qu’une technologie précise.

===

## Limites

* Certains calculs ne sont pas parallélisables. Exemple: Suite de Fibonacci
* MapReduce utilise difficilement des index.

===

## Le machine learning

* But du jeu: faire « apprendre » la machine.
* N’est possible que lorsque l’on possède énormément de données concernant un problème.
* (Il existe des moyens d’estimer le seuil critique à atteindre mais nous n’en discuterons pas ici).

===

## Le machine learning

* En machine learning, nous n’avons qu’une masse impressionnante d’inputs et d’outputs:  x1 -> y1 x2 -> y2 x3 -> y3 (…)
* Et l’on cherche à trouver la fonction qui permet de passer de x à y:  f(x) = y -> Que fait f?

===

## Le machine learning

* Exemple:  x1 = 3 -> y1 = 5, x2 = 1 -> y2 = 3, x3 = 7 -> y3 = 9. 
* Sans challenge: f(x) = x + 2
* Imaginons cela sur des données mille fois plus complexes et nombreuses et nous aurons une idée à peu près correcte de ce qu’est le machine learning dans sa plus simple expression.

===

## Le perceptron

* Inventé en 1957 par Franck Rosenblatt.
* Le plus ‘simple’ des algorithmes de machine learning (pondération linéaire).
* Il est dit « itératif » car il fonctionne par itérations successives jusqu’à ce que l’on considère son résultat satisfaisant (ce qui peut potentiellement ne jamais arriver).
* Contrairement à un algorithme déterministe qui a une fin (qui peut être très longue à atteindre, par ailleurs).

===

## Le perceptron

===

## Exemple

* L’OCR (Optical Character Recognition)
* 1) Séparer les charactères
* 2) Analyser leur structure.

===

## Encore des exemples?

* 1) La banque et les prêts.
* 2) Netflix et son système de recommandation.

===

## Le deep learning

* Famille d’algorithme appartenant aussi à la famille du machine learning.
* S’inspirer du fonctionnement du cerveau humain.
* Certains considèrent que ce ne sont que des réseaux de neurones en embuscade (retour sur le perceptron).
* Algorithme DQN (DeepMind).
* Encore une fois, itératif contre déterministe (plus ignorance des règles).

===

## Quelques outils

* OpenOffice, Excel, Tableau, Stata, OpenRefine, Gephi
* AWS, Google BigQuery (étonnant, non?)

===

## Marché du travail

* Profils recherchés:  - Data scientists  - Développeurs barbus  - CDP spécialisés
* Pour le design, on verra dans les prochains cours.

===

## Data scientist

* Métier aux contours encore très flou.
* Tout le monde cherche des data scientists sans toujours savoir ce que c’est.
* Charactéristique d’un bon data scientist: - Bonne bases en développement backend/script - Mathématiques et statistiques - Sensibilité business (logique métier du client)
* Souvent proche des métiers en analytics mais avec la programmation en plus.

===

## Programmation

* PHP? Non.
* Langages de script: Python, Ruby, Node/Io.js
* Java ou un langage compilant vers la JVM: Clojure, Scala, Groovy.
* Langages ‘scientifiques’: Matlab, R, Julia etc.
* Langages compilés: C, C++, Go etc.
* Langage fonctionnel: Lisp, Clojure, Haskell, Erlang etc.
* Développer pour le Big Data, c’est être fiable  et, évidemment, scalable.

===

## Beware!

* Le métier de data scientist peut paraître sexy mais:
* Avez-vous déjà passé un mois à nettoyer des données immondes pour ne rien y trouver au final?
* Inutile de vous dire que si vous n’aimez pas le traitement de données, vous n’êtes pas fait pour ça.

===

## Entreprises

* Laboratoires de recherche
* Linkfluence, MFG Labs, Orange innovation.
* Géants de l’Internet.

===

## Teaser

* Dans les prochaines semaines, nous allons nous pencher sur:  - Le web mining, le crawling, le scraping etc. - La collecte et le traitement de données. - L’Open Data - La visualisation de données.

===

## Quelle orientation?

* Des sujets vous intéressent-ils particulièrement?
* Souhaitez-vous faire un TP concernant une technologie ou une pratique en particulier?

===

## Contact

* N’hésitez pas si vous avez des questions ou des difficultés, notamment en programmation dans des projets concernant les sujets du cours:
* Mail: guillaume.plique@sciencespo.fr
* Github: @Yomguithereal

